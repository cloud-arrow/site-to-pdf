#!/usr/bin/env python3
"""
HTTrack ç½‘ç«™è½¬ PDF å·¥å…· (ä½¿ç”¨ Playwright)
å°† HTTrack æŠ“å–çš„ç½‘ç«™è½¬æ¢ä¸ºå•ä¸ª PDF æ–‡ä»¶
"""

import sys
import argparse
import asyncio
import tempfile
import logging
from pathlib import Path
from bs4 import BeautifulSoup
from PyPDF2 import PdfMerger
from playwright.async_api import async_playwright
from typing import List, Dict, Set
from urllib.parse import unquote

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SiteToPDF:
    def __init__(self, 
                 httrack_dir: str, 
                 output_pdf: str = "website.pdf",
                 browser_type: str = "chromium",
                 max_pages: int = None,
                 include_toc: bool = True,
                 hide_sidebar: bool = True):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            httrack_dir: HTTrack è¾“å‡ºç›®å½•
            output_pdf: è¾“å‡º PDF æ–‡ä»¶å
            browser_type: æµè§ˆå™¨ç±»å‹ (chromium/firefox/webkit)
            max_pages: æœ€å¤§è½¬æ¢é¡µé¢æ•°
            include_toc: æ˜¯å¦åŒ…å«ç›®å½•
            hide_sidebar: æ˜¯å¦éšè—ä¾§è¾¹æ å’Œå¯¼èˆªæ 
        """
        self.httrack_dir = Path(httrack_dir).resolve()
        self.output_pdf = output_pdf
        self.browser_type = browser_type
        self.max_pages = max_pages
        self.include_toc = include_toc
        self.hide_sidebar = hide_sidebar
        
        self.html_files: List[Path] = []
        self.page_info: Dict[str, Dict] = {}
        self.visited_pages: Set[str] = set()
        
    def find_html_files(self) -> List[Path]:
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰ HTML æ–‡ä»¶"""
        logger.info("æ­£åœ¨æœç´¢ HTML æ–‡ä»¶...")
        
        html_patterns = ['*.html', '*.htm']
        html_files = []
        
        for pattern in html_patterns:
            html_files.extend(self.httrack_dir.rglob(pattern))
        
        # è¿‡æ»¤æ’é™¤é¡¹
        exclude_patterns = [
            'hts-cache', 'hts-log', 
            'backblue.gif', 'fade.gif',
            'index.html~',  # ä¸´æ—¶æ–‡ä»¶
            '/404',  # 404 é”™è¯¯é¡µé¢
        ]
        
        self.html_files = [
            f for f in html_files 
            if not any(pattern in str(f) for pattern in exclude_patterns)
        ]
        
        logger.info(f"æ‰¾åˆ° {len(self.html_files)} ä¸ª HTML æ–‡ä»¶")
        return self.html_files
    
    def analyze_page_structure(self):
        """åˆ†æé¡µé¢ç»“æ„å’Œé“¾æ¥å…³ç³»"""
        logger.info("åˆ†æé¡µé¢é“¾æ¥å…³ç³»...")
        
        for html_file in self.html_files:
            try:
                with open(html_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    soup = BeautifulSoup(content, 'html.parser')
                
                # æå–é¡µé¢ä¿¡æ¯
                title = soup.find('title')
                title_text = title.get_text().strip() if title else html_file.stem
                
                # æå–å…ƒæ•°æ®
                meta_description = soup.find('meta', attrs={'name': 'description'})
                description = meta_description.get('content', '') if meta_description else ''
                
                # æå–æ‰€æœ‰å†…éƒ¨é“¾æ¥
                links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    # æ¸…ç†é“¾æ¥
                    href = unquote(href.split('#')[0].split('?')[0])
                    
                    if href and (href.endswith(('.html', '.htm')) or href.endswith('/')):
                        links.append(href)
                
                # æå–é¡µé¢å±‚çº§ï¼ˆåŸºäºè·¯å¾„æ·±åº¦ï¼‰
                relative_path = html_file.relative_to(self.httrack_dir)
                depth = len(relative_path.parts) - 1
                
                self.page_info[str(html_file)] = {
                    'title': title_text,
                    'description': description,
                    'links': list(set(links)),  # å»é‡
                    'file': html_file,
                    'depth': depth,
                    'size': html_file.stat().st_size,
                    'relative_path': str(relative_path)
                }
                
            except Exception as e:
                logger.error(f"è§£ææ–‡ä»¶ {html_file} æ—¶å‡ºé”™: {e}")
    
    def find_start_page(self) -> str:
        """æŸ¥æ‰¾ç½‘ç«™é¦–é¡µ"""
        # ä¼˜å…ˆçº§é¡ºåº
        index_names = [
            'index.html', 'index.htm', 
            'home.html', 'home.htm',
            'default.html', 'default.htm',
            'main.html', 'main.htm'
        ]
        
        # åœ¨æ ¹ç›®å½•æŸ¥æ‰¾
        for name in index_names:
            index_file = self.httrack_dir / name
            if index_file.exists():
                logger.info(f"æ‰¾åˆ°é¦–é¡µ: {name}")
                return str(index_file)
        
        # æŸ¥æ‰¾æœ€çŸ­è·¯å¾„çš„ index æ–‡ä»¶
        index_files = [f for f in self.html_files if 'index' in f.name.lower()]
        if index_files:
            shortest = min(index_files, key=lambda x: len(str(x)))
            logger.info(f"ä½¿ç”¨é¦–é¡µ: {shortest.name}")
            return str(shortest)
        
        # è¿”å›æ·±åº¦æœ€å°çš„æ–‡ä»¶
        if self.html_files:
            sorted_files = sorted(
                self.html_files, 
                key=lambda x: len(x.relative_to(self.httrack_dir).parts)
            )
            logger.info(f"ä½¿ç”¨é¦–é¡µ: {sorted_files[0].name}")
            return str(sorted_files[0])
        
        return None
    
    def build_page_tree(self) -> List[str]:
        """æ„å»ºé¡µé¢æ ‘ç»“æ„ï¼ˆä½¿ç”¨ä¾§è¾¹æ é¡ºåºæˆ–å¹¿åº¦ä¼˜å…ˆéå†ï¼‰"""
        start_page = self.find_start_page()
        if not start_page:
            return [str(f) for f in self.html_files]
        
        # å°è¯•ä»é¦–é¡µæå–ä¾§è¾¹æ é“¾æ¥é¡ºåº
        ordered_pages = self._extract_sidebar_order(start_page)
        
        if ordered_pages:
            logger.info(f"ä½¿ç”¨ä¾§è¾¹æ é¡ºåºï¼Œæ‰¾åˆ° {len(ordered_pages)} ä¸ªé¡µé¢")
            # æ·»åŠ ä»»ä½•æœªåœ¨ä¾§è¾¹æ ä¸­çš„é¡µé¢
            for page_path in self.page_info.keys():
                if page_path not in ordered_pages:
                    ordered_pages.append(page_path)
        else:
            # å›é€€åˆ°å¹¿åº¦ä¼˜å…ˆéå†
            logger.info("ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆéå†")
            ordered_pages = []
            queue = [start_page]
            self.visited_pages.clear()
            
            while queue and (not self.max_pages or len(ordered_pages) < self.max_pages):
                current_page = queue.pop(0)
                
                if current_page in self.visited_pages:
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not Path(current_page).exists():
                    current_page = self._resolve_page_path(current_page)
                    if not current_page:
                        continue
                
                self.visited_pages.add(current_page)
                ordered_pages.append(current_page)
                
                # æ·»åŠ é“¾æ¥é¡µé¢åˆ°é˜Ÿåˆ—
                if current_page in self.page_info:
                    for link in self.page_info[current_page]['links']:
                        linked_page = self._resolve_link(current_page, link)
                        if linked_page and linked_page not in self.visited_pages:
                            queue.append(linked_page)
            
            # æ·»åŠ æœªè®¿é—®çš„é¡µé¢
            for page_path in self.page_info.keys():
                if page_path not in ordered_pages:
                    if not self.max_pages or len(ordered_pages) < self.max_pages:
                        ordered_pages.append(page_path)
        
        logger.info(f"é¡µé¢æ ‘æ„å»ºå®Œæˆï¼Œå…± {len(ordered_pages)} ä¸ªé¡µé¢")
        return ordered_pages
    
    def _extract_sidebar_order(self, index_page: str) -> List[str]:
        """ä»é¦–é¡µä¾§è¾¹æ æå–é¡µé¢é¡ºåº"""
        try:
            with open(index_page, 'r', encoding='utf-8', errors='ignore') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
            
            ordered_pages = [index_page]  # é¦–é¡µæ”¾åœ¨ç¬¬ä¸€ä½
            
            # æŸ¥æ‰¾ä¾§è¾¹æ é“¾æ¥
            sidebar = soup.find('aside', class_='sidebar')
            if not sidebar:
                sidebar = soup.find('ul', class_='sidebar-links')
            
            if sidebar:
                # æŒ‰é¡ºåºæå–æ‰€æœ‰é“¾æ¥
                for link in sidebar.find_all('a', class_='sidebar-link'):
                    href = link.get('href', '')
                    if href and not href.startswith(('http://', 'https://', '#')):
                        # è§£æç›¸å¯¹è·¯å¾„
                        linked_page = self._resolve_link(index_page, href)
                        if linked_page and linked_page not in ordered_pages:
                            ordered_pages.append(linked_page)
                
                return ordered_pages
            
            return []
        except Exception as e:
            logger.warning(f"æ— æ³•æå–ä¾§è¾¹æ é¡ºåº: {e}")
            return []
    
    def _resolve_link(self, current_page: str, link: str) -> str:
        """è§£æé“¾æ¥ä¸ºç»å¯¹è·¯å¾„"""
        current_dir = Path(current_page).parent
        
        # å¤„ç†ç›¸å¯¹é“¾æ¥
        if not link.startswith(('http://', 'https://')):
            # ç§»é™¤å‰å¯¼ ./
            link = link.lstrip('./')
            
            if link.startswith('/'):
                # ç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºç½‘ç«™æ ¹ç›®å½•ï¼‰
                linked_file = self.httrack_dir / link.lstrip('/')
            else:
                # ç›¸å¯¹è·¯å¾„
                linked_file = current_dir / link
            
            # å¤„ç†ç›®å½•é“¾æ¥
            if link.endswith('/'):
                linked_file = linked_file / 'index.html'
            
            # è§„èŒƒåŒ–è·¯å¾„
            try:
                linked_file = linked_file.resolve()
                if linked_file.exists() and linked_file.is_file():
                    return str(linked_file)
            except Exception:
                pass
        
        return None
    
    def _resolve_page_path(self, page_path: str) -> str:
        """å°è¯•è§£æé¡µé¢è·¯å¾„"""
        # å°è¯•æ·»åŠ  index.html
        if page_path.endswith('/'):
            index_path = Path(page_path) / 'index.html'
            if index_path.exists():
                return str(index_path)
        
        return None
    
    async def html_to_pdf(self, page, html_file: str, pdf_file: str) -> bool:
        """ä½¿ç”¨ Playwright å°† HTML è½¬æ¢ä¸º PDF - å®Œå…¨æ¨¡æ‹Ÿæµè§ˆå™¨æ‰“å°"""
        try:
            # æå‰è®¾ç½®æ›´å®½è§†å£ï¼Œå‡å°‘å¸ƒå±€æ¢è¡Œå¯¼è‡´çš„æˆªæ–­
            try:
                await page.set_viewport_size({"width": 2048, "height": 1400})
            except Exception:
                pass

            # æ¨¡æ‹Ÿæ‰“å°ä»‹è´¨ï¼ˆè®©æµè§ˆå™¨é‡‡ç”¨ print åª’ä½“è§„åˆ™ï¼‰
            try:
                await page.emulate_media(media="print")
            except Exception:
                pass

            # åŠ è½½æœ¬åœ°æ–‡ä»¶å¹¶ç­‰å¾…ç½‘ç»œç©ºé—²
            file_url = Path(html_file).as_uri()
            await page.goto(file_url, wait_until='networkidle', timeout=60000)

            # ç­‰ä¸»è¦å†…å®¹æ¸²æŸ“
            try:
                await page.wait_for_selector('.theme-default-content, .content__default, main', timeout=10000)
            except Exception:
                pass

            # å…œåº•æ‰“å°æ ·å¼ï¼šç»ˆææ¨åœŸæœºç­–ç•¥ - å¼ºåˆ¶æ‰€æœ‰å†…å®¹å¯è§å’Œæ¢è¡Œ
            css_rules = [
                "@page { size: A4; margin: 10mm 10mm 12mm 10mm; }",
                # å…¨å±€è¦†ç›– - å¼ºåˆ¶æ‰€æœ‰å…ƒç´ å¯è§
                "* { overflow: visible !important; max-width: none !important; }",
                "html, body { width: auto !important; overflow: visible !important; }",
                "body, .theme-container, main, .page, .theme-default-content, div, section { max-width: none !important; width: auto !important; }",
                "img, svg, video, canvas { max-width: 100% !important; height: auto !important; }",
                ".theme-container { padding-left: 0 !important; padding-right: 0 !important; }",
                "main.page, .page, main { margin: 0 !important; padding: 0 8mm !important; }",
                ".theme-default-content, .content__default { width: 100% !important; padding: 0 !important; }",
                # ä»£ç å—å¼ºåˆ¶æ¢è¡Œ
                "pre, pre code, code { white-space: pre-wrap !important; word-break: break-all !important; overflow-wrap: break-word !important; }",
                # è¡¨æ ¼ - ä¿è¯å†…å®¹å®Œæ•´ä½†ä¸è¿‡åº¦æ‹†åˆ†å•è¯
                "table { table-layout: auto !important; width: 100% !important; border-collapse: collapse !important; overflow: visible !important; }",
                "table td, table th { white-space: normal !important; overflow-wrap: break-word !important; overflow: visible !important; padding: 6px !important; font-size: 12px !important; line-height: 1.5 !important; }",
                "table td code, table th code { white-space: pre-wrap !important; word-break: break-all !important; }",
                # é¿å…å›ºå®šå®šä½å…ƒç´ è¦†ç›–å†…å®¹
                "[style*='position:fixed'], [style*='position: fixed'] { display: none !important; }",
                # é¿å…åœ¨å…³é”®å…ƒç´ å†…éƒ¨åˆ†é¡µ
                "tr, pre, code, figure { page-break-inside: avoid !important; }",
            ]

            if self.hide_sidebar:
                css_rules.append(".sidebar, aside.sidebar, .sidebar-mask, .navbar, header.navbar, nav, .page-edit, .page-nav, .search-box, .sidebar-button, .global-ui { display: none !important; }")

            await page.add_style_tag(content="\n".join(css_rules))

            # ç­‰æ ·å¼åº”ç”¨
            await asyncio.sleep(0.5)

            # å¯¼å‡ºä¸º A4ï¼Œä¿ç•™èƒŒæ™¯ï¼›æ›´å°çš„ç¼©æ”¾ä»¥å®¹çº³æ›´å¤šå†…å®¹
            await page.pdf(
                path=pdf_file,
                print_background=True,
                prefer_css_page_size=True, # ä¼˜å…ˆä½¿ç”¨ @page
                scale=0.85
            )
            return True
        except Exception as e:
            logger.error(f"è½¬æ¢ {Path(html_file).name} æ—¶å‡ºé”™: {e}")
            return False
    
    def generate_toc_html(self, page_order: List[str]) -> str:
        """ç”Ÿæˆç›®å½•é¡µé¢ï¼ˆå¸¦å±‚çº§ç»“æ„ï¼‰"""
        total_pages = len(page_order)
        toc_html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ç›®å½•</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            max-width: 900px;
            margin: 40px auto;
            padding: 20px;
        }}
        h1 {{
            color: #333;
            border-bottom: 3px solid #4CAF50;
            padding-bottom: 10px;
        }}
        .toc-item {{
            margin: 8px 0;
            padding: 8px 10px;
            border-left: 3px solid #ddd;
            transition: all 0.2s;
        }}
        .toc-item:hover {{
            background-color: #f5f5f5;
            border-left-color: #4CAF50;
        }}
        .toc-item.depth-0 {{
            margin-left: 0;
            border-left-color: #4CAF50;
            background-color: #f9f9f9;
        }}
        .toc-item.depth-1 {{
            margin-left: 20px;
            border-left-color: #2196F3;
        }}
        .toc-item.depth-2 {{
            margin-left: 40px;
            border-left-color: #FF9800;
        }}
        .toc-item.depth-3 {{
            margin-left: 60px;
            border-left-color: #9C27B0;
        }}
        .toc-item.depth-4 {{
            margin-left: 80px;
        }}
        .page-number {{
            color: #666;
            font-weight: bold;
            margin-right: 10px;
            min-width: 30px;
            display: inline-block;
        }}
        .page-title {{
            color: #333;
            font-size: 15px;
            font-weight: 500;
        }}
        .page-path {{
            color: #999;
            font-size: 11px;
            margin-top: 4px;
            font-family: monospace;
        }}
        .depth-indicator {{
            color: #bbb;
            margin-right: 5px;
            font-size: 12px;
        }}
    </style>
</head>
<body>
    <h1>ğŸ“š ç½‘ç«™ç›®å½•</h1>
    <p>å…± <strong>{total_pages}</strong> ä¸ªé¡µé¢</p>
    <hr>
"""
        
        for i, page_path in enumerate(page_order, 1):
            info = self.page_info.get(page_path, {})
            title = info.get('title', Path(page_path).name)
            relative_path = info.get('relative_path', '')
            depth = info.get('depth', 0)
            
            # é™åˆ¶æœ€å¤§æ·±åº¦æ˜¾ç¤º
            display_depth = min(depth, 4)
            
            # ç”Ÿæˆå±‚çº§æŒ‡ç¤ºç¬¦
            indent_symbol = "â””â”€ " if depth > 0 else ""
            
            toc_html += f"""    <div class="toc-item depth-{display_depth}">
        <span class="page-number">{i}.</span>
        <span class="depth-indicator">{indent_symbol}</span>
        <span class="page-title">{title}</span>
        <div class="page-path">{relative_path}</div>
    </div>
"""
        
        toc_html += """</body>
</html>
"""
        return toc_html
    
    async def convert(self):
        """æ‰§è¡Œå®Œæ•´çš„è½¬æ¢æµç¨‹"""
        logger.info("å¼€å§‹è½¬æ¢ç½‘ç«™ä¸º PDF...")
        
        # 1. æŸ¥æ‰¾ HTML æ–‡ä»¶
        if not self.find_html_files():
            logger.error("æœªæ‰¾åˆ° HTML æ–‡ä»¶")
            return False
        
        # 2. åˆ†æé¡µé¢ç»“æ„
        self.analyze_page_structure()
        
        # 3. æ„å»ºé¡µé¢æ ‘
        page_order = self.build_page_tree()
        
        if not page_order:
            logger.error("æ²¡æœ‰å¯è½¬æ¢çš„é¡µé¢")
            return False
        
        logger.info(f"å‡†å¤‡è½¬æ¢ {len(page_order)} ä¸ªé¡µé¢")
        
        # 4. ä½¿ç”¨ Playwright è½¬æ¢
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            logger.info(f"å¯åŠ¨ {self.browser_type} æµè§ˆå™¨...")
            
            if self.browser_type == "chromium":
                browser = await p.chromium.launch(headless=True)
            elif self.browser_type == "firefox":
                browser = await p.firefox.launch(headless=True)
            elif self.browser_type == "webkit":
                browser = await p.webkit.launch(headless=True)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æµè§ˆå™¨ç±»å‹: {self.browser_type}")
                return False
            
            page = await browser.new_page()
            
            # 5. åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                pdf_files = []
                
                # ç”Ÿæˆç›®å½•é¡µ
                if self.include_toc:
                    logger.info("ç”Ÿæˆç›®å½•é¡µ...")
                    toc_html = self.generate_toc_html(page_order)
                    toc_file = Path(temp_dir) / "toc.html"
                    toc_file.write_text(toc_html, encoding='utf-8')
                    
                    toc_pdf = Path(temp_dir) / "000_toc.pdf"
                    if await self.html_to_pdf(page, str(toc_file), str(toc_pdf)):
                        pdf_files.append(str(toc_pdf))
                
                # 6. é€ä¸ªè½¬æ¢é¡µé¢
                for i, page_path in enumerate(page_order, 1):
                    page_name = Path(page_path).name
                    logger.info(f"[{i}/{len(page_order)}] è½¬æ¢: {page_name}")
                    
                    pdf_file = Path(temp_dir) / f"page_{i:04d}.pdf"
                    
                    if await self.html_to_pdf(page, page_path, str(pdf_file)):
                        pdf_files.append(str(pdf_file))
                    else:
                        logger.warning(f"è·³è¿‡é¡µé¢: {page_name}")
                
                await browser.close()
                
                # 7. åˆå¹¶ PDF
                if pdf_files:
                    logger.info(f"åˆå¹¶ {len(pdf_files)} ä¸ª PDF æ–‡ä»¶...")
                    return self.merge_pdfs(pdf_files)
                else:
                    logger.error("æ²¡æœ‰æˆåŠŸè½¬æ¢çš„ PDF æ–‡ä»¶")
                    return False
    
    def merge_pdfs(self, pdf_files: List[str]) -> bool:
        """åˆå¹¶å¤šä¸ª PDF æ–‡ä»¶"""
        try:
            merger = PdfMerger()
            
            for pdf_file in pdf_files:
                if Path(pdf_file).exists():
                    merger.append(pdf_file)
            
            merger.write(self.output_pdf)
            merger.close()
            
            logger.info(f"âœ… PDF å·²ä¿å­˜è‡³: {self.output_pdf}")
            
            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            size_mb = Path(self.output_pdf).stat().st_size / (1024 * 1024)
            logger.info(f"æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
            
            return True
            
        except Exception as e:
            logger.error(f"åˆå¹¶ PDF æ—¶å‡ºé”™: {e}")
            return False


async def main():
    parser = argparse.ArgumentParser(
        description='å°† HTTrack æŠ“å–çš„ç½‘ç«™è½¬æ¢ä¸º PDF (Playwright ç‰ˆæœ¬)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s /path/to/httrack/output
  %(prog)s /path/to/httrack/output -o website.pdf
  %(prog)s /path/to/httrack/output -o site.pdf -b firefox --max-pages 50
  %(prog)s /path/to/httrack/output --no-toc --keep-sidebar
        """
    )
    
    parser.add_argument('httrack_dir', help='HTTrack è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('-o', '--output', default='website.pdf', 
                       help='è¾“å‡º PDF æ–‡ä»¶å (é»˜è®¤: website.pdf)')
    parser.add_argument('-b', '--browser', 
                       choices=['chromium', 'firefox', 'webkit'],
                       default='chromium',
                       help='æµè§ˆå™¨ç±»å‹ (é»˜è®¤: chromium)')
    parser.add_argument('--max-pages', type=int, 
                       help='æœ€å¤§è½¬æ¢é¡µé¢æ•°')
    parser.add_argument('--no-toc', action='store_true',
                       help='ä¸ç”Ÿæˆç›®å½•é¡µ')
    parser.add_argument('--keep-sidebar', action='store_true',
                       help='ä¿ç•™ä¾§è¾¹æ å’Œå¯¼èˆªæ ï¼ˆé»˜è®¤éšè—ï¼‰')
    
    args = parser.parse_args()
    
    if not Path(args.httrack_dir).exists():
        logger.error(f"é”™è¯¯: ç›®å½• {args.httrack_dir} ä¸å­˜åœ¨")
        sys.exit(1)
    
    converter = SiteToPDF(
        args.httrack_dir, 
        args.output,
        browser_type=args.browser,
        max_pages=args.max_pages,
        include_toc=not args.no_toc,
        hide_sidebar=not args.keep_sidebar
    )
    
    success = await converter.convert()
    
    if success:
        logger.info("è½¬æ¢å®Œæˆ!")
    else:
        logger.error("è½¬æ¢å¤±è´¥!")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
